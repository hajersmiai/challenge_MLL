{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0df04b7b",
   "metadata": {},
   "source": [
    "# Limitations with Large Language Models\n",
    "\n",
    "Large Language Models are objectively great. They are flexible, surprisingly cunning, and have a considerable amount of knowledge by themselves. They do come short in some cases, especially when it comes to adapting to new contextual information. Let's say you're trying to build an LLM that answers all the questions you may have about BeCode rules. What does ChatGPT know about BeCode rules, was there any of it in its training data? Probably not much.\n",
    "\n",
    "## How could we have a LLM answer BeCode questions?\n",
    "\n",
    "An LLM has a very long context window, as of the writing of this notebook close to 1 million words for ChatGPT, that means close to two books from Game of Thrones can be given to it and it would still be able to answer. We could give it all of BeCode rules as a text in the prompt and have it answer questions based on them. But it still comes with many caveats, mostly that giving a lot of content to an LLM is quite costly in resources and money.\n",
    "\n",
    "Wouldn't it be better if we could just give it the parts of the document useful in the answer to help it on the prompt at hand? The LLM doesn't need to be told about the way moodle works in order to explain when the holidays of the bootcamp happen. The document with Becode Rules is given in `data/becode_rules.txt`.\n",
    "\n",
    "Make it so that the LLM can answer the following prompt by giving it the paragraph from becode_rules that will allow it to answer the prompt, insert this in the code snippet underneath:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a098d83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      " Use the following snippet:\n",
      " #üìùMoodle\n",
      "\n",
      "You have to go on moodle every day 4 times (ideally in these slots):\n",
      "9:00, morning check-in\n",
      "12:30, morning check-out\n",
      "13:30, afternoon check-in\n",
      "17:00, afternoon check-out\n",
      "\n",
      "Check-in, up to 10 minutes before the time. For example, morning check in can happen from 8:50 to 9:00. \n",
      "Check out up to 10 minutes after the time. For example, morning check out can happen from 12:30 to 12:40.\n",
      "\n",
      "If you forget to check-in or check-out, warn your coach. DO NOT check-in whenever you are not on campus or in a discord room. This will be considered unjustified absence.\n",
      "\n",
      "#üí¨Discord\n",
      "\n",
      "Discord is where we have our home working days happen.\n",
      "\n",
      "When you are in discord, you have to be on a table. You can mute yourself, but you cannot deafen yourself because people have to be able to reach you.\n",
      "\n",
      "Tech talks happen in the main-room channel. Download: https://discord.com/download\n",
      "\n",
      "Do not use the @everyone tag because it will notify everyone on the server (approx 500 people). If you want to tag everyone just tag your class (example @Thomas5).\n",
      "\n",
      "Discord link: [Insert Discord Link Here]\n",
      "\n",
      "#üåêGoogle\n",
      "\n",
      "We organise ourselves through a google drive folder and a google calendar. The links are:\n",
      "To the calendar, giving you schedule and distribution of on-campus days: [Insert Calendar Link Here]\n",
      "To the calendar giving you informations about Events related to data:\n",
      "https://calendar.google.com/calendar/embed?src=c_d7e422ddda419a9d4c78654a6024bcbf8af6b7697903e707f20826c7871315aa%40group.calendar.google.com&ctz=Europe%2FBrussels\n",
      "\n",
      "#üí°Tech Talks\n",
      "\n",
      "You can make a tech talk about any tech topic you want, example subjects:\n",
      "- What is AI image generation\n",
      "- How does AI learn\n",
      "- Quiz on Python\n",
      "- What is web development\n",
      "- Data Analysis vs Data Engineering\n",
      "(Invite a friend who works in the field to explain his job)\n",
      "\n",
      "These are just silly examples. You can do whichever one you want as long as it is related to tech. The schedule can be found in this spreadsheet which you can edit: [Insert Tech Talk SpreadSheet Link Here]\n",
      "\n",
      "#üå°Ô∏èYou cannot attend class\n",
      "\n",
      "If you cannot attend class for any reason, usually when you are sick. There are 2 steps:\n",
      "Warn your day coach (main coach or co-coach) and campus coordinator by email. Example emails:\n",
      "main.coach@becode.org\n",
      "co.coach@becode.org\n",
      "campus.coordinator@becode.org\n",
      "You have to have a justification paper, upload it to moodle. If it is too late to upload it, send it to your campus coordinator and your main coach.\n",
      "\n",
      "#üíºInternships\n",
      "\n",
      "Internships at BeCode Wallonia last between 1 and 3 months. In Brussels it's between 1 and 2 months.\n",
      "Earliest start date is the day the summer pastures start.\n",
      "Latest end date is 3 months after the end of the training.\n",
      "When you find someone interested to take you in for an internship. The first steps to follow are first to fill in the Projet de Stage.docx document. Make sure all the fields are filled. And then send it to your campus coordinator as well as to your main coach by email.\n",
      "\n",
      "Once the projet de stage has been approved, you may request the link to an online form service called sphinx in which you will have to copy the details of the approved projet de stage. Once this is done, the document will be sent to be signed electronically.\n",
      "\n",
      "#üå¥Holidays\n",
      "\n",
      "There will be two main holiday weeks during the bootcamp:\n",
      "Holiday at the end of the Hill\n",
      "Holiday towards the end of the Mountain\n",
      "We might also give some long weekends if it suits well.\n",
      "\n",
      " To answer this question: when the holidays of the bootcamp happen?\n",
      "\n",
      "\n",
      "Answer:\n",
      " The bootcamp has two main holiday weeks:\n",
      "\n",
      "*   At the end of the Hill phase.\n",
      "*   Towards the end of the Mountain phase.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os, re, textwrap, pathlib\n",
    "from google import genai\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the environment\n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "# Check if the API key is loaded properly\n",
    "if not API_KEY:\n",
    "    raise Exception('GEMINI_API_KEY not found. Please set it in your .env file.')\n",
    "\n",
    "# Use the API key\n",
    "client = genai.Client(api_key=API_KEY)\n",
    "\n",
    "question = 'when the holidays of the bootcamp happen?'\n",
    "# Path to the BeCode rules file (relative to the notebook folder)\n",
    "rules_path = pathlib.Path(\"data\") / \"becode_rules.txt\"\n",
    "if not rules_path.exists():\n",
    "    # fallback if you're running from repo root:\n",
    "    alt = pathlib.Path(\"03-TheMountain/DataScience/GenAIText/data/becode_rules.txt\")\n",
    "    rules_path = alt if alt.exists() else rules_path\n",
    "if not rules_path.exists():\n",
    "    raise FileNotFoundError(f\"Cannot find becode_rules.txt at {rules_path.absolute()}\")\n",
    "context = rules_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "prompt = f'Use the following snippet:\\n {context}\\n\\n To answer this question: {question}'\n",
    "print(\"Prompt:\\n\",prompt)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-lite\", contents=prompt\n",
    ")\n",
    "print(\"\\n\\nAnswer:\\n\", response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9361b266",
   "metadata": {},
   "source": [
    "## Word2Vec makes a comeback\n",
    "\n",
    "You just had to give the answer to ChatGPT to have it tell you what to do. Not very handy, might as well go search in the document yourself. But what if there was a way to make a program perform that search automatically?\n",
    "\n",
    "In the previous notebooks, you may have read that we used to turn some words into vectors to encode meaning about them, and that words with similar meanings had similar vectors. Well what if you could do this instead with many words? What if you could do it with paragraphs? Wouldn't that be great.\n",
    "\n",
    "Well as it turns out, you can, you can make [embeddings for paragraphs](https://ai.google.dev/gemini-api/docs/embeddings). You can do that with an entire document, and then use the paragraph who's vectors are similar to your prompt to augment it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "921c55df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ContentEmbedding(\n",
      "  values=[\n",
      "    -5.8070676e-05,\n",
      "    0.009062129,\n",
      "    -0.010935352,\n",
      "    -0.06167249,\n",
      "    -0.033063993,\n",
      "    <... 3067 more items ...>,\n",
      "  ]\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "prompt_embed = client.models.embed_content(\n",
    "        model=\"gemini-embedding-001\",\n",
    "        contents='when the holidays of the bootcamp happen?')\n",
    "\n",
    "print(prompt_embed.embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb637791",
   "metadata": {},
   "source": [
    "Edit this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1efb6fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#üìùMoodle You have to go on moodle every day 4 times (ideally in these slots): 9:00, morning check-in 12:30, morning check-out 13:30, afternoon check-in 17:00, afternoon check-out Check-in, up to 10 minutes before the time. For example, morning check in can happen from 8:50 to 9:00. Check out up to 10 minutes after the time. For example, morning check out can happen from 12:30 to 12:40. If you forget to check-in or check-out, warn your coach. DO NOT check-in whenever you are not on campus or in a', ' discord room. This will be considered unjustified absence. #üí¨Discord Discord is where we have our home working days happen. When you are in discord, you have to be on a table. You can mute yourself, but you cannot deafen yourself because people have to be able to reach you. Tech talks happen in the main-room channel. Download: https://discord.com/download Do not use the @everyone tag because it will notify everyone on the server (approx 500 people). If you want to tag everyone just tag your cla', 'ss (example @Thomas5). Discord link: [Insert Discord Link Here] #üåêGoogle We organise ourselves through a google drive folder and a google calendar. The links are: To the calendar, giving you schedule and distribution of on-campus days: [Insert Calendar Link Here] To the calendar giving you informations about Events related to data: https://calendar.google.com/calendar/embed?src=c_d7e422ddda419a9d4c78654a6024bcbf8af6b7697903e707f20826c7871315aa%40group.calendar.google.com&ctz=Europe%2FBrussels #üí°', 'Tech Talks You can make a tech talk about any tech topic you want, example subjects: - What is AI image generation - How does AI learn - Quiz on Python - What is web development - Data Analysis vs Data Engineering (Invite a friend who works in the field to explain his job) These are just silly examples. You can do whichever one you want as long as it is related to tech. The schedule can be found in this spreadsheet which you can edit: [Insert Tech Talk SpreadSheet Link Here] #üå°Ô∏èYou cannot attend', \" class If you cannot attend class for any reason, usually when you are sick. There are 2 steps: Warn your day coach (main coach or co-coach) and campus coordinator by email. Example emails: main.coach@becode.org co.coach@becode.org campus.coordinator@becode.org You have to have a justification paper, upload it to moodle. If it is too late to upload it, send it to your campus coordinator and your main coach. #üíºInternships Internships at BeCode Wallonia last between 1 and 3 months. In Brussels it'\", 's between 1 and 2 months. Earliest start date is the day the summer pastures start. Latest end date is 3 months after the end of the training. When you find someone interested to take you in for an internship. The first steps to follow are first to fill in the Projet de Stage.docx document. Make sure all the fields are filled. And then send it to your campus coordinator as well as to your main coach by email. Once the projet de stage has been approved, you may request the link to an online form ', 'service called sphinx in which you will have to copy the details of the approved projet de stage. Once this is done, the document will be sent to be signed electronically. #üå¥Holidays There will be two main holiday weeks during the bootcamp: Holiday at the end of the Hill Holiday towards the end of the Mountain We might also give some long weekends if it suits well.']\n"
     ]
    }
   ],
   "source": [
    "doc_content=\"\"\n",
    "with open('data/becode_rules.txt','r',encoding=\"utf-8\") as f:\n",
    "    doc_content=f.read()\n",
    "\n",
    "chunk_size=500\n",
    "chunks=[]\n",
    "\n",
    "# Normalize whitespace\n",
    "text = re.sub(r\"\\s+\", \" \", doc_content).strip()\n",
    "# Split the document in chunks of 500 Characters and put it in the chunks array so that we have\n",
    "for i in range(0, len(text), chunk_size):\n",
    "    chunks.append(text[i:i + chunk_size])\n",
    "\n",
    "print(chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cf28ea",
   "metadata": {},
   "source": [
    "Run this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15713394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 3:\n",
      "Tech Talks You can make a tech talk about any tech topic you want, example subjects: - What is AI image generation - How does AI learn - Quiz on Python - What is web development - Data Analysis vs Data Engineering (Invite a friend who works in the field to explain his job) These are just silly examples. You can do whichever one you want as long as it is related to tech. The schedule can be found in this spreadsheet which you can edit: [Insert Tech Talk SpreadSheet Link Here] #üå°Ô∏èYou cannot attend\n",
      "\n",
      "Chunk 4:\n",
      " class If you cannot attend class for any reason, usually when you are sick. There are 2 steps: Warn your day coach (main coach or co-coach) and campus coordinator by email. Example emails: main.coach@becode.org co.coach@becode.org campus.coordinator@becode.org You have to have a justification paper, upload it to moodle. If it is too late to upload it, send it to your campus coordinator and your main coach. #üíºInternships Internships at BeCode Wallonia last between 1 and 3 months. In Brussels it'\n",
      "\n",
      "Similarity between the prompt and Chunk 3: 0.5757\n",
      "Similarity between the prompt and Chunk 4: 0.6180\n",
      "Is this giving you any ideas?\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Prompt embedding\n",
    "prompt_vector = np.array(prompt_embed.embeddings[0].values).reshape(1, -1)\n",
    "\n",
    "# Chunk 3 and 4 embeddings\n",
    "chunk_3_vector = np.array(\n",
    "    client.models.embed_content(\n",
    "        model=\"gemini-embedding-001\",\n",
    "        contents=chunks[3]\n",
    "    ).embeddings[0].values\n",
    ").reshape(1, -1)\n",
    "\n",
    "chunk_4_vector = np.array(\n",
    "    client.models.embed_content(\n",
    "        model=\"gemini-embedding-001\",\n",
    "        contents=chunks[4]\n",
    "    ).embeddings[0].values\n",
    ").reshape(1, -1)\n",
    "\n",
    "# Print the text\n",
    "print(\"Chunk 3:\")\n",
    "print(chunks[3])\n",
    "\n",
    "print(\"\\nChunk 4:\")\n",
    "print(chunks[4])\n",
    "\n",
    "# Compute cosine similarities\n",
    "sim_3 = cosine_similarity(prompt_vector, chunk_3_vector)[0][0]\n",
    "sim_4 = cosine_similarity(prompt_vector, chunk_4_vector)[0][0]\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nSimilarity between the prompt and Chunk 3: {sim_3:.4f}\")\n",
    "print(f\"Similarity between the prompt and Chunk 4: {sim_4:.4f}\")\n",
    "\n",
    "print(\"Is this giving you any ideas?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1b1cd4",
   "metadata": {},
   "source": [
    "Take time to understand the example given above. Then try to make a system which can answer any question by selecting the best chunks from the content to answer the prompt. You may explore different chunk sizes, some overlap between chunks and a criterion for minimum required similarity. Give it a try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f84d5d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top retrieved chunks:\n",
      "\n",
      "[1] score=0.6469\n",
      "class If you cannot attend class for any reason, usually when you are sick. There are 2 steps: Warn your day coach (main coach or co-coach) and campus coordinator by email. Example emails: main.coach@becode.org co.coach@becode.org campus.coordinator@becode.org You have to [...]\n",
      "\n",
      "[2] score=0.5250\n",
      "#üìùMoodle You have to go on moodle every day 4 times (ideally in these slots): 9:00, morning check-in 12:30, morning check-out 13:30, afternoon check-in 17:00, afternoon check-out Check-in, up to 10 minutes before the time. For example, morning check in can happen from 8:50 [...]\n",
      "\n",
      "[3] score=0.4693\n",
      "discord room. This will be considered unjustified absence. #üí¨Discord Discord is where we have our home working days happen. When you are in discord, you have to be on a table. You can mute yourself, but you cannot deafen yourself because people have to be able to reach you. [...]\n",
      "\n",
      "\n",
      "================================================================================\n",
      "FINAL ANSWER\n",
      "================================================================================\n",
      "You should upload your justification paper to Moodle. If it's too late, send it to your campus coordinator and main coach.\n"
     ]
    }
   ],
   "source": [
    "#Your code here\n",
    "from google import genai\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import os, textwrap\n",
    "\n",
    "# ----------------- Setup -----------------\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise RuntimeError(\"Set GEMINI_API_KEY in your .env\")\n",
    "\n",
    "client = genai.Client(api_key=API_KEY)\n",
    "\n",
    "EMBED_MODEL = \"text-embedding-004\"   # or \"gemini-embedding-001\" if that's what you have\n",
    "GEN_MODEL   = \"gemini-2.0-flash-lite\"\n",
    "\n",
    "# ----------------- Load BeCode rules -----------------\n",
    "doc_content=\"\"\n",
    "with open('data/becode_rules.txt','r',encoding=\"utf-8\") as f:\n",
    "    doc_content=f.read()\n",
    "question = \"I am sick, I sent an email to my main coach and my campus coordinator, what else should I do?\"\n",
    "\n",
    "# ----------------- Helpers -----------------\n",
    "def to_vec(resp) -> np.ndarray:\n",
    "    \"\"\"Normalize embedding response to a 1D float32 vector.\"\"\"\n",
    "    if hasattr(resp, \"embedding\") and hasattr(resp.embedding, \"values\"):\n",
    "        vals = resp.embedding.values\n",
    "    elif hasattr(resp, \"embeddings\") and resp.embeddings and hasattr(resp.embeddings[0], \"values\"):\n",
    "        vals = resp.embeddings[0].values\n",
    "    else:\n",
    "        raise RuntimeError(f\"Unexpected embedding response: {type(resp)} -> {resp}\")\n",
    "    return np.asarray(vals, dtype=np.float32)\n",
    "\n",
    "def embed_text(txt: str) -> np.ndarray:\n",
    "    r = client.models.embed_content(model=EMBED_MODEL, contents=txt)\n",
    "    return to_vec(r)\n",
    "\n",
    "# ----------------- Retrieval -----------------\n",
    "# Embed question\n",
    "q_vec = embed_text(question).reshape(1, -1)\n",
    "\n",
    "# Embed all chunks (use the chunks list you already built)\n",
    "chunk_vecs = [embed_text(c) for c in chunks]\n",
    "mat = np.stack(chunk_vecs, axis=0)          # (N, D)\n",
    "\n",
    "# Rank chunks by cosine similarity\n",
    "sims = cosine_similarity(q_vec, mat)[0]     # (N,)\n",
    "top_k = 3\n",
    "top_idx = sims.argsort()[::-1][:top_k]\n",
    "\n",
    "print(\"Top retrieved chunks:\\n\")\n",
    "for r, i in enumerate(top_idx, 1):\n",
    "    print(f\"[{r}] score={sims[i]:.4f}\")\n",
    "    print(textwrap.shorten(chunks[i].replace(\"\\n\", \" \"), width=280))\n",
    "    print()\n",
    "\n",
    "# Optional: gate by a minimum similarity so we avoid hallucination\n",
    "MIN_SIM = 0.35\n",
    "selected = [i for i in top_idx if sims[i] >= MIN_SIM]\n",
    "if not selected:\n",
    "    print(\"No chunk passes the similarity threshold. I will answer: 'I don't know based on the rules.'\")\n",
    "    selected = top_idx[:1]\n",
    "\n",
    "context = \"\\n\\n---\\n\\n\".join(chunks[i] for i in selected)\n",
    "\n",
    "# ----------------- Generation -----------------\n",
    "grounded_prompt = f\"\"\"\n",
    "Answer STRICTLY from the provided BeCode rules context. \n",
    "If the answer is not in the context, say you don't know.\n",
    "\n",
    "# Context:\n",
    "{context}\n",
    "\n",
    "# Question:\n",
    "{question}\n",
    "\n",
    "Provide a concise, actionable answer. If relevant, cite or paraphrase the specific rule.\n",
    "\"\"\".strip()\n",
    "\n",
    "resp = client.models.generate_content(\n",
    "    model=GEN_MODEL,\n",
    "    contents=[{\"role\": \"user\", \"parts\": [{\"text\": grounded_prompt}]}],\n",
    ")\n",
    "\n",
    "# Robust text extraction\n",
    "def extract_text(r):\n",
    "    try:\n",
    "        return r.candidates[0].content.parts[0].text.strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "    for attr in (\"text\", \"output_text\"):\n",
    "        if hasattr(r, attr) and getattr(r, attr):\n",
    "            return getattr(r, attr).strip()\n",
    "    return str(r)\n",
    "\n",
    "answer = extract_text(resp)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL ANSWER\")\n",
    "print(\"=\"*80)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eba503",
   "metadata": {},
   "source": [
    "## Where is this going\n",
    "\n",
    "I hope you'll quickly realise how powerful this approach can be when augmenting the prompts of LLMS. You just need to find the segments with the highest similarity and feed them in the prompt. This is the beauty of Retrieval Augmented Generation (RAG). There are many things to explore from here and I invite you to go look for these topics in whichever way you prefer. Here is an example list of ideas:\n",
    "\n",
    "- You can look into different embedding models for sentences and paragraphs. This is the current overall [leaderboard](https://huggingface.co/spaces/mteb/leaderboard)\n",
    "- You can look into ways google has of using sentence embeddings which may be a little less clunky than what we did above\n",
    "- You can have a peek into ways of augmenting RAG with a classic keywords search\n",
    "- You can try to replicate this example with something you care about\n",
    "\n",
    "Have fun with it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv312 (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
